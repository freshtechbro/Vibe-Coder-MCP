import fs from 'fs-extra';
import path from 'path';
import logger from '../logger.js'; // Assuming logger is correctly set up
import { OpenRouterConfig } from '../types/workflow.js'; // Import OpenRouterConfig type

/**
 * Interface for the structure of the LLM configuration file.
 */
interface LlmConfigFile {
  llm_mapping: Record<string, string>;
}

/**
 * Loads the LLM model mapping configuration from a JSON file and merges with .env preferences.
 * 
 * @param fileName The name of the configuration file. Defaults to 'llm_config.json'.
 * @returns An object containing the llm_mapping, with .env overrides applied.
 */
export function loadLlmConfigMapping(
  fileName: string = 'llm_config.json'
): Record<string, string> {
  let filePath: string | null = null;
  let baseMapping: Record<string, string> = {};

  // 1. Check Environment Variable
  if (process.env.LLM_CONFIG_PATH) {
    const envPath = process.env.LLM_CONFIG_PATH;
    if (fs.existsSync(envPath)) {
      logger.info(`Found LLM config path in environment variable: ${envPath}`);
      filePath = envPath;
    } else {
      logger.warn(`LLM_CONFIG_PATH environment variable set to ${envPath}, but file not found.`);
    }
  }

  // 2. Fallback to Current Working Directory if env var didn't work
  if (!filePath) {
    const cwdPath = path.join(process.cwd(), fileName);
    if (fs.existsSync(cwdPath)) {
      logger.info(`Found LLM config in current working directory: ${cwdPath}`);
      filePath = cwdPath;
    }
  }

  // Load base mapping from file
  if (filePath) {
    try {
      const fileContent = fs.readFileSync(filePath, 'utf-8');
      const parsedConfig = JSON.parse(fileContent) as LlmConfigFile;

      if (parsedConfig && typeof parsedConfig.llm_mapping === 'object' && parsedConfig.llm_mapping !== null) {
        logger.info(`LLM config loaded successfully from ${filePath}`);
        // Validate that values are strings (basic check)
        for (const key in parsedConfig.llm_mapping) {
          if (typeof parsedConfig.llm_mapping[key] !== 'string') {
             logger.warn(`Invalid non-string value found for key "${key}" in ${filePath}. Skipping this key.`);
             delete parsedConfig.llm_mapping[key]; // Remove invalid entry
          }
        }
        baseMapping = parsedConfig.llm_mapping;
      } else {
        logger.error(`Invalid structure in ${filePath}. Expected 'llm_mapping' object.`);
      }
    } catch (error) {
      logger.error({ err: error, filePath }, `Failed to load or parse LLM config from ${filePath}.`);
    }
  }

  // 3. Apply .env overrides for ALL model preferences
  const envOverrides: Record<string, string> = {};
  
  // Get model from env or use default fallback
  const getModelFromEnv = (envVar: string, fallback: string): string => {
    return process.env[envVar] || fallback;
  };

  // Define all models from .env
  const models = {
    default: getModelFromEnv('DEFAULT_MODEL', 'deepseek/deepseek-r1-0528-qwen3-8b:free'),
    deepseek: getModelFromEnv('DEEPSEEK_MODEL', 'deepseek/deepseek-r1-0528-qwen3-8b:free'),
    qwen: getModelFromEnv('QWEN_MODEL', 'qwen/qwen3-30b-a3b:free'),
    kimi: getModelFromEnv('KIMI_MODEL', 'moonshotai/kimi-dev-72b:free'),
    llama: getModelFromEnv('LLAMA_MODEL', 'meta-llama/llama-3.3-70b-instruct:free'),
    deepseekV3: getModelFromEnv('DEEPSEEK_V3_MODEL', 'deepseek/deepseek-v3-base:free'),
    minimax: getModelFromEnv('MINIMAX_MODEL', 'minimax/minimax-m1:free'),
    nvidia: getModelFromEnv('NVIDIA_MODEL', 'nvidia/llama-3.3-nemotron-super-49b-v1:free'),
    gemini: getModelFromEnv('GEMINI_MODEL', 'google/gemini-2.5-flash-preview-05-20'),
    perplexity: getModelFromEnv('PERPLEXITY_MODEL', 'perplexity/sonar-deep-research')
  };

  // Apply models to specific task categories
  
  // Code-focused tasks use KIMI (software engineering optimized)
  const codeTasks = [
    'dependency_analysis',
    'context_curator_intent_analysis',
    'context_curator_file_discovery',
    'context_curator_task_decomposition',
    'rules_generation',
    'task_validation',
    'project_analysis',
    'capability_matching'
  ];
  codeTasks.forEach(task => {
    envOverrides[task] = models.kimi;
  });

  // General generation tasks use LLAMA (strong general performance)
  const generalTasks = [
    'prd_generation',
    'user_stories_generation',
    'task_list_initial_generation',
    'task_list_decomposition',
    'fullstack_starter_kit_generation',
    'fullstack_starter_kit_module_selection',
    'fullstack_starter_kit_dynamic_yaml_module_generation',
    'context_curator_prompt_refinement',
    'context_curator_relevance_scoring',
    'context_curator_meta_prompt_generation',
    'context_curator_architectural_analysis',
    'agent_response_processing',
    'task_orchestration',
    'epic_generation',
    'session_persistence',
    'artifact_parsing',
    'prd_integration',
    'task_list_integration',
    'default_generation'
  ];
  generalTasks.forEach(task => {
    envOverrides[task] = models.llama;
  });

  // Reasoning tasks use DEEPSEEK (reasoning optimized)
  const reasoningTasks = [
    'sequential_thought_generation',
    'intent_recognition',
    'atomic_task_detection',
    'dependency_graph_analysis',
    'research_query_generation',
    'research_enhancement',
    'agent_task_assignment',
    'agent_status_analysis',
    'agent_health_monitoring',
    'transport_optimization',
    'error_recovery_analysis',
    'orchestration_workflow',
    'natural_language_processing',
    'command_parsing',
    'task_refinement',
    'agent_coordination',
    'workflow_step_execution'
  ];
  reasoningTasks.forEach(task => {
    envOverrides[task] = models.deepseek;
  });

  // Research stays with Perplexity
  envOverrides['research_query'] = models.perplexity;

  // 4. Merge base mapping with environment overrides
  const finalMapping = { ...baseMapping, ...envOverrides };
  
  logger.info({
    baseConfigTasks: Object.keys(baseMapping).length,
    envOverrides: Object.keys(envOverrides).length,
    finalTasks: Object.keys(finalMapping).length,
    modelsUsed: {
      kimi: models.kimi,
      llama: models.llama,
      deepseek: models.deepseek,
      perplexity: models.perplexity
    }
  }, 'LLM config loading complete with .env overrides');

  return finalMapping;
}

/**
 * Selects the appropriate LLM model based on task type and available mappings
 *
 * @param config The OpenRouter configuration with mappings
 * @param logicalTaskName The logical task name (e.g., 'research_query')
 * @param defaultModel The default model to use if no mapping is found
 * @returns The selected model name
 */
export function selectModelForTask(
  config: OpenRouterConfig,
  logicalTaskName: string,
  defaultModel: string
): string {
  // Log the received config object *before* any copying or modification
  logger.debug({
    receivedConfig: config,
    receivedMapping: config?.llm_mapping,
    taskName: logicalTaskName
  }, 'selectModelForTask received config');

  // Ensure config and llm_mapping exist before proceeding
  const mapping = config?.llm_mapping;
  if (!mapping || typeof mapping !== 'object') {
    logger.warn({ logicalTaskName, configProvided: !!config }, `LLM mapping object is missing or invalid in provided config. Falling back to default model: ${defaultModel}`);
    return defaultModel;
  }

  // Check if the mapping object is empty
  const mappingKeys = Object.keys(mapping);
  if (mappingKeys.length === 0) {
     logger.warn({ logicalTaskName }, `LLM mapping object is empty. Falling back to default model: ${defaultModel}`);
     return defaultModel;
  }

  // Log the mapping lookup details
  const modelFromMapping = mapping[logicalTaskName];
  const defaultFromMapping = mapping['default_generation'];
  logger.debug({
    logicalTaskName,
    mappingKeys: mappingKeys,
    modelFromMapping: modelFromMapping,
    defaultFromMapping: defaultFromMapping,
    defaultModelProvided: defaultModel
  }, `Looking up model for task: ${logicalTaskName}`);


  // Select model with priority: Task Specific -> Default Mapping -> Default Model Param
  const modelToUse = modelFromMapping || defaultFromMapping || defaultModel;

  // Log the final model selection at INFO level for better visibility
  logger.info(
    { logicalTaskName, modelFromMapping, defaultFromMapping, defaultModel, modelToUse },
    'Model selection decision for task'
  );

  return modelToUse;
}